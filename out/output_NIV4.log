nohup: ignoring input
Namespace(checkpoint_root='/data/zhaobo/zhouyufan/PDPP-Optimize/checkpoint', checkpoint_max_root='/data/zhaobo/zhouyufan/PDPP-Optimize/save_max', log_root='/data/zhaobo/zhouyufan/PDPP-Optimize/log/log', checkpoint_dir='whl', optimizer='adam', num_thread_reader=8, batch_size=256, batch_size_val=256, momemtum=0.9, save_freq=1, crop_only=1, centercrop=0, random_flip=1, verbose=1, fps=1, cudnn_benchmark=1, dataset='NIV', action_dim=48, observation_dim=1536, class_dim=5, root='/data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV', json_path_train='/data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV/train70.json', json_path_val='/data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV/test30.json', json_path_val2='/data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV/output4.json', n_train_steps=50, start_epoch=0, resume=False, resume_path='', evaluate=True, pretrained=False, pin_memory=True, world_size=1, rank=0, dist_file='dist-file', dist_backend='nccl', multiprocessing_distributed=False, name='NIV10', loss_type='Weighted_Gradient_MSE', ckpt_path='', dist_port=21712, log_freq=500, gpu=4, seed=217, weight=6, clip_denoised=True, ddim_discr_method='uniform', lr=0.0003, ema_decay=0.995, gradient_accumulate_every=1, step_start_ema=400, update_ema_every=10, ie_num=2, transformer_num=5, base_model='base', classfier_model='transformer', n_diffusion_steps=50, num_heads=4, num_layers=2, dim_feedforward=1024, dropout=0.4, horizon=4, epochs=0, if_jump=1, distributed=False)
/data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV/train70_4.json
Loaded /data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV/train70_4.json
/data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV/output4.json
Loaded /data/zhaobo/zhouyufan/PDPP-Optimize/dataset/NIV/output4.json
logging outputs to  /data/zhaobo/zhouyufan/PDPP-Optimize/log/log_NIV10_20240828211244_NIV
total train:   0%|          | 0/130 [00:00<?, ?it/s]lrs:
3.3333333333333333e-06
---------------------------------
/data/zhaobo/zhouyufan/PDPP-Optimize/utils/eval.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(A0_acc.avg), torch.tensor(AT_acc.avg)
total train:   1%|          | 1/130 [05:14<11:16:55, 314.85s/it]0.43668121099472046 0
lrs:
6.666666666666667e-06
---------------------------------
total train:   2%|▏         | 2/130 [11:01<11:51:12, 333.38s/it]3.4934496879577637 0.43668121099472046
lrs:
9.999999999999999e-06
---------------------------------
total train:   2%|▏         | 3/130 [17:52<13:01:24, 369.17s/it]6.986899375915527 3.4934496879577637
lrs:
1.3333333333333333e-05
---------------------------------
total train:   3%|▎         | 4/130 [24:55<13:39:31, 390.25s/it]11.35371208190918 3.4934496879577637
lrs:
1.6666666666666664e-05
---------------------------------
total train:   4%|▍         | 5/130 [31:49<13:50:52, 398.82s/it]8.733624458312988 11.35371208190918
lrs:
1.9999999999999998e-05
---------------------------------
total train:   5%|▍         | 6/130 [38:42<13:54:13, 403.66s/it]12.663755416870117 11.35371208190918
