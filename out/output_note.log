nohup: ignoring input
Namespace(checkpoint_root='/home/zhouyufan/Projects/PDPP/checkpoint', checkpoint_max_root='/home/zhouyufan/Projects/PDPP/save_max', log_root='/home/zhouyufan/Projects/PDPP/log/log_mlp', checkpoint_dir='whl', optimizer='adam', num_thread_reader=8, batch_size=256, batch_size_val=256, pretrain_cnn_path='', momemtum=0.9, save_freq=1, crop_only=1, centercrop=0, random_flip=1, verbose=1, fps=1, cudnn_benchmark=1, dataset='crosstask_base', action_dim=105, observation_dim=9600, class_dim=18, root='/home/zhouyufan/Projects/PDPP/dataset/crosstask', json_path_train='/home/zhouyufan/Projects/PDPP/dataset/crosstask/crosstask_release/base/train_list.json', json_path_val='/home/zhouyufan/Projects/PDPP/dataset/crosstask/crosstask_release/base/test_list.json', json_path_val2='/home/zhouyufan/Projects/PDPP/dataset/crosstask/crosstask_release/base/output.json', n_train_steps=200, start_epoch=0, resume=False, evaluate=True, pretrained=False, pin_memory=True, world_size=1, rank=0, dist_file='dist-file', dist_backend='nccl', gpu=6, multiprocessing_distributed=False, name='note', loss_type='Weighted_Gradient_MSE', ckpt_path='', dist_port=21712, horizon=3, epochs=70, log_freq=500, seed=217, weight=6, n_diffusion_steps=200, clip_denoised=True, ddim_discr_method='uniform', lr=0.0005, ema_decay=0.995, gradient_accumulate_every=1, step_start_ema=400, update_ema_every=10, imageEncoder=0, transformer_num=5, base_model='base')
crosstask_base
feature_base
is_valFalse
Loaded /home/zhouyufan/Projects/PDPP/dataset/crosstask/crosstask_release/base/train_list.json
crosstask_base
feature_base
is_valTrue
Loaded /home/zhouyufan/Projects/PDPP/dataset/crosstask/crosstask_release/base/test_list.json
logging outputs to  /home/zhouyufan/Projects/PDPP/log/log_mlp_note_20240729175722_crosstask_base
total_mlp:   0%|          | 0/70 [00:00<?, ?it/s]total_mlp:   1%|▏         | 1/70 [00:51<58:39, 51.00s/it]lrs:
8.571428571428573e-06
---------------------------------
/home/zhouyufan/Projects/PDPP/train_mlp.py:473: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(losses.avg), torch.tensor(acc)
total_mlp:   3%|▎         | 2/70 [02:01<1:10:35, 62.29s/it]total_mlp:   4%|▍         | 3/70 [02:51<1:03:36, 56.97s/it]lrs:
2.5714285714285714e-05
---------------------------------
total_mlp:   6%|▌         | 4/70 [03:53<1:04:52, 58.98s/it]total_mlp:   7%|▋         | 5/70 [04:43<1:00:06, 55.48s/it]lrs:
4.2857142857142856e-05
---------------------------------
total_mlp:   9%|▊         | 6/70 [05:45<1:01:41, 57.84s/it]total_mlp:  10%|█         | 7/70 [06:36<58:25, 55.64s/it]  lrs:
6e-05
---------------------------------
total_mlp:  11%|█▏        | 8/70 [07:31<57:13, 55.38s/it]total_mlp:  13%|█▎        | 9/70 [08:21<54:28, 53.58s/it]lrs:
7.714285714285714e-05
---------------------------------
total_mlp:  14%|█▍        | 10/70 [09:13<53:07, 53.13s/it]